syntax = "proto3";

package yandex.cloud.dataproc.v1;

import "google/protobuf/timestamp.proto";

option go_package = "github.com/yandex-cloud/go-genproto/yandex/cloud/dataproc/v1;dataproc";
option java_package = "yandex.cloud.api.dataproc.v1";
option java_outer_classname = "PHJ";

// Dataproc job.
message Job {
  // Required. Unique ID of the Dataproc job.
  // This ID is assigned by MDB in the process of creating Dataproc job.
  string id = 1;

  // Required. Unique ID of the Dataproc cluster.
  string cluster_id = 2;

  // The time when the Dataproc job was created.
  google.protobuf.Timestamp created_at = 3;

  // The time when the Dataproc job was started.
  google.protobuf.Timestamp started_at = 4;

  // The time when the Dataproc job was finished.
  google.protobuf.Timestamp finished_at = 5;

  // Name of the Dataproc job.
  string name = 6;

  enum Status {
    STATUS_UNSPECIFIED = 0;
    PROVISIONING = 1; // Job created in metadb and is waiting agent to acquire.
    PENDING = 2; // Job acquired by agent and is waiting for execution.
    RUNNING = 3; // Job is running.
    ERROR = 4; // Job failed.
    DONE = 5; // Job finished.
  }
  // Status.
  Status status = 7;

  // Job specification.
  oneof job_spec {
    MapreduceJob mapreduce_job = 8;
    SparkJob spark_job = 9;
    PysparkJob pyspark_job = 10;
    HiveJob hive_job = 11;
  }
}

message MapreduceJob {
  // Optional arguments to the driver.
  repeated string args = 1;

  // URIs of file to run.
  repeated string jar_file_uris = 2;

  // URIs of files to be copied to the working directory of Dataproc drivers and distributed tasks.
  repeated string file_uris = 3;

  // URIs of archives to be extracted in the working directory of Dataproc drivers and tasks.
  repeated string archive_uris = 4;

  // A mapping of property names to values, used to configure Dataproc.
  map<string, string> properties = 5;

  oneof driver {
    // The HCFS URI of the jar file containing the main class.
    string main_jar_file_uri = 6;

    // The name of the driver's main class.
    string main_class = 7;
  }
}

message SparkJob {
  // Optional arguments to the driver.
  repeated string args = 1;

  // Jar file URIs to add to the CLASSPATHs of the Dataproc driver and tasks.
  repeated string jar_file_uris = 2;

  // URIs of files to be copied to the working directory of Dataproc drivers and distributed tasks.
  repeated string file_uris = 3;

  // URIs of archives to be extracted in the working directory of Dataproc drivers and tasks.
  repeated string archive_uris = 4;

  // A mapping of property names to values, used to configure Dataproc.
  map<string, string> properties = 5;

  // The HCFS URI of the jar file containing the main class.
  string main_jar_file_uri = 6;

  // The name of the driver's main class.
  string main_class = 7;
}

message PysparkJob {
  // Optional arguments to the driver.
  repeated string args = 1;

  // Jar file URIs to add to the CLASSPATHs of the Dataproc driver and tasks.
  repeated string jar_file_uris = 2;

  // URIs of files to be copied to the working directory of Dataproc drivers and distributed tasks.
  repeated string file_uris = 3;

  // URIs of archives to be extracted in the working directory of Dataproc drivers and tasks.
  repeated string archive_uris = 4;

  // A mapping of property names to values, used to configure Dataproc.
  map<string, string> properties = 5;

  // URI of the main Python file to use as the driver. Must be a .py file.
  string main_python_file_uri = 6;

  // URIs of Python files to pass to the PySpark framework.
  repeated string python_file_uris = 7;
}

message QueryList {
  repeated string queries = 1;
}

message HiveJob {
  // A mapping of property names to values, used to configure Hive.
  map<string, string> properties = 1;

  // Whether to continue executing queries if a query fails.
  bool continue_on_failure = 2;

  // Mapping of query variable names to values.
  map<string, string> script_variables = 3;

  // Jar file URIs to add to the CLASSPATHs of the Hive driver and tasks.
  repeated string jar_file_uris = 4;

  oneof query_type {
    // URI of the script that contains Hive queries.
    string query_file_uri = 5;

    QueryList query_list = 6;
  }
}
